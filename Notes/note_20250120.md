# Replication and extension

When the likelihood is tractable

- MLE
  - Correctly specified
  - Misspecified (Pseudo MLE)
- SMM
- AdE
  - logistic D
    - oracle
    - correctly specified 
    - wrongly specified
  - Neural network D

We still consider logistic location model where the true distribution is the standard logistic distribution.

$$ p_0(x)=\Lambda(x)(1-\Lambda(x)) $$

## MLE

### Correctly specified

$$ p_{\theta}(x)=\Lambda(x-\theta)(1-\Lambda(x-\theta)) $$

### Misspecified (assume to be normal)

$$ p_{\theta}(x)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{(x-\theta)^2}{2}\right) $$

## AdE

### Logistic D (oracle)

$$ D_\text{oracle}(x) = \Lambda(-\theta-2\log(1+e^{-x})+2\log(1+e^{-(x-\theta)})) $$

### Logistic D (correctly specified)
$$ D(x;\lambda) = \Lambda(\lambda_0 -2\log(1+e^{-x})+2\log(1+e^{-x+\lambda_1}))$$

### Logistic D (wrongly specified)

**Version 1**
$$ D(x;\lambda) = \Lambda(\lambda_0 +\lambda_1 x + \lambda_2 x^2+\lambda_3 \log(1+e^{-x}))$$

**Version 2**
$$ D(x;\lambda) = \Lambda(\lambda_0 +\lambda_1 x + \lambda_2 x^2+\lambda_3 x^3 + \ldots + \lambda_d x^d) \quad d=3,7,11$$

### Neural network D
> To see how a nonparametric discriminator fares, we also try a shallow neural network discriminator. The input is a one-dimensional observation X; there are three nodes in one hidden layer with a hyperbolic tangent activation function; the output is a sigmoid function.

## SMM

$$ \mathbb{E}(X_i), \mathbb{E}(X_i^2),\ldots \mathbb{E}(X_i^d) \quad d=3,7,11$$

## Figures

### Likelihood
1. MLE (true)
2. Oracle D (true)
3. Correctly specified D (trained)
4. Neural network D (trained) 
5. Misspecified D (true and trained)
6. quasi-MLE (true)

- Figure 1: 1 & 2 & 3
- Figure 2: 1 & 2 & 4
- Figure 3: 6 & 5 (the misspecified D has two versions: the truly misspecified and the trained)
- I want to compare: 2 & 3 & 4
### Moments
1. Oracle true D (What is the oracle D?) (true)
2. Misspecified D (The logistic location model with increasing numbers of inputs. The curvature of the cross-entropy loss is very close to the log likelihood up to 7 moments and is still good for 11 moments.) (trained)
3. SMM loglikelihood (true)
4. Neural network D (trained)

- Figure 4: 1 & 2 & 3
- I want to compare: 1 & 4 & 3

